{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "HaystackAIP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrUm6dVkEqKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install git+https://github.com/deepset-ai/haystack.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVXZF_KEEqKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from haystack import Finder\n",
        "from haystack.indexing.cleaning import clean_wiki_text\n",
        "from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.reader.transformers import TransformersReader\n",
        "from haystack.utils import print_answers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOqkn6SsEqKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vb3e1aDEqKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing Elasticsearch\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.6.2\n",
        "\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYhgVkUkEqKY",
        "colab_type": "code",
        "colab": {},
        "outputId": "a3ea8751-1cdf-489d-d91e-f7db1f04a470"
      },
      "source": [
        "# Connect to Elasticsearch\n",
        "from haystack.database.elasticsearch import ElasticsearchDocumentStore\n",
        "\n",
        "# We need to set `embedding_field` and `embedding_dim`, when we plan to use a dense retriever\n",
        "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\", \n",
        "                                            embedding_field=\"embedding\", embedding_dim=768)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/02/2020 09:08:33 - INFO - elasticsearch -   PUT http://localhost:9200/document [status:400 request:0.009s]\n",
            "08/02/2020 09:08:33 - INFO - elasticsearch -   PUT http://localhost:9200/label [status:400 request:0.009s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnvi9O5VEqKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recording task types from csv\n",
        "queries = pd.read_csv('data/questions/allQuestions.csv') \n",
        "queries['PageNum'] = pd.Series(queries['PageNum']).fillna(method='ffill')\n",
        "queries['Context'] = pd.Series(queries['Context']).fillna(method='ffill')\n",
        "taskType = {}\n",
        "for index in range(len(queries['Question'])):\n",
        "  # Set each question as a key, and the task type as its value\n",
        "  taskType[queries['Question'][index]] = queries['TaskType'][index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzKz5SBeEqKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open(\"data/questions/squadQA.json\",\"r+\") \n",
        "string = file1.read()\n",
        "mydict = json.loads(string)\n",
        "strings = mydict['data'][0]['paragraphs']\n",
        "numTest = int(math.floor(0.5 * len(task)))\n",
        "#Generate indices to extract test data\n",
        "random.seed(5)\n",
        "randomlist = random.sample(range(len(task)), numTest)\n",
        "test = {}\n",
        "index = 0\n",
        "for row in range(len(strings)):\n",
        "  #Iterate through the questions for each context\n",
        "  qas = strings[row]['qas']\n",
        "  qa = 0\n",
        "  while qa < len(qas):\n",
        "    if index in randomlist:\n",
        "      test[qas[qa]['question']] = qas[qa]['answers'][0]['text']\n",
        "      del qas[qa]\n",
        "      #qa should remain at the same value during next iteration\n",
        "      qa -= 1\n",
        "    qa += 1\n",
        "    index += 1\n",
        "with open('data/questions/trainqa.json','w') as f:\n",
        "  json.dump(mydict,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsCMFZpDEqKg",
        "colab_type": "code",
        "colab": {},
        "outputId": "c9e65924-f576-43ce-f3bf-5b267944abf3"
      },
      "source": [
        "#Read in corpus\n",
        "file1 = open(\"data/cleaned_text/summary.txt\",\"r+\",encoding='UTF-8') \n",
        "string = file1.read()\n",
        "paragraphs =string.split('.\\n')\n",
        "list_par=[]\n",
        "for p in range(len(paragraphs)):\n",
        "  #Remove headers and footers\n",
        "  #if ('©' in paragraphs[p]) or ('AIP SingaporeGEN' in paragraphs[p]):\n",
        "    #continue\n",
        "  #turn lists into sentences by replacing - with ,\n",
        "  paragraphs[p] = paragraphs[p].replace(';\\n -',',')\n",
        "  #first element of list does not require a comma\n",
        "  paragraphs[p]= paragraphs[p].replace('\\n -','')\n",
        "  paragraphs[p]=paragraphs[p].replace(';','')\n",
        "  result = paragraphs[p].split('\\n')\n",
        "  for r in range(len(result)):\n",
        "    #Remove whitespace between lines\n",
        "    result[r] = result[r].strip()\n",
        "    #Remove whitespace between words\n",
        "    words = result[r].split(' ')\n",
        "    s = ' '\n",
        "    result[r] = s.join([word for word in words if word != ''])\n",
        "  paragraphs[p] = s.join(result)\n",
        "  paragraphs[p]=paragraphs[p].replace('\\n','')\n",
        "  paragraphs[p] = paragraphs[p].replace('°',' degrees ')\n",
        "  list_par.append(paragraphs[p].strip())\n",
        "sep = '. '\n",
        "passage = sep.join(list_par)\n",
        "dict1= [{'meta':{'name':'summary.txt'},'text':passage}]\n",
        "document_store.write_documents(dict1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/02/2020 09:09:20 - INFO - elasticsearch -   POST http://localhost:9200/_bulk?refresh=wait_for [status:200 request:0.862s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDgue_yEqKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize retriever\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "retriever = DensePassageRetriever(document_store=document_store, embedding_model=\"dpr-bert-base-nq\",\n",
        "                                  do_lower_case=True, use_gpu=True)\n",
        "document_store.update_embeddings(retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEUnphmvEqKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader = FARMReader(model_name_or_path=\"ktrapeznikov/albert-xlarge-v2-squad-v2\", use_gpu=True)\n",
        "reader2 = FARMReader(model_name_or_path=\"ahotrod/electra_large_discriminator_squad2_512\", use_gpu=True)\n",
        "#reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
        "#reader2 = FARMReader(model_name_or_path=\"deepset/bert-base-cased-squad2\", use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAE7sqxdEqKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reader.train(data_dir='mydata', train_filename=\"trainqa.json\",save_dir=\"models/albert\",num_processes=128, n_epochs=1,use_gpu=True)\n",
        "new_reader = FARMReader(model_name_or_path=\"models/albert\")\n",
        "finder = Finder(new_reader, retriever)\n",
        "\n",
        "reader2.train(data_dir='mydata', train_filename=\"trainqa.json\",save_dir=\"models/electra\",num_processes=128, n_epochs=1,use_gpu=True)\n",
        "new_reader2 = FARMReader(model_name_or_path=\"models/electra\")\n",
        "finder2 = Finder(new_reader2, retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbGqVFIgEqKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "def best_prediction(predictions, ground_truth):\n",
        "    f1 = 0\n",
        "    ans = ''\n",
        "    for pred in predictions['answers']:\n",
        "        if f1_score(pred['answer'],ground_truth) >= f1:\n",
        "            f1 = f1_score(pred['answer'],ground_truth)\n",
        "            ans = pred['answer']\n",
        "    return (ans,f1)\n",
        "def ensemble_prediction(input1,input2,ground_truth):\n",
        "    weights = [0.5744746037802809,0.5471326379596643]\n",
        "    output = []\n",
        "    prediction1 = input1['answers']\n",
        "    prediction2 = input2['answers']\n",
        "    #Select the most likely prediction, weighted by the f1-score of each model\n",
        "    if (weights[0]*prediction1[0]['score']) > (weights[1]*prediction2[0]['score']):\n",
        "        top =  prediction1.pop(0)\n",
        "    else:\n",
        "        top = prediction2.pop(0)\n",
        "    output = [[top['answer'],top['score']],\n",
        "              [prediction1[0]['answer'],prediction1[0]['score']],\n",
        "              [prediction2[0]['answer'],prediction2[0]['score']]]\n",
        "    f1 = 0\n",
        "    ans = ''\n",
        "    for o in output:\n",
        "        if f1_score(o[0],ground_truth) >= f1:\n",
        "            f1 = f1_score(o[0],ground_truth)\n",
        "            ans = o[0]\n",
        "    return (ans,f1)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br_jj4-uEqKp",
        "colab_type": "code",
        "colab": {},
        "outputId": "c00fe8be-15ec-4eea-de81-879e39042e19"
      },
      "source": [
        "#Single query for testing purposes\n",
        "qn = 'What is the normal permit fee for 10 return flights?'\n",
        "input1 = finder.get_answers(question=qn, top_k_retriever=30, top_k_reader=3)\n",
        "input2 = finder2.get_answers(question=qn, top_k_retriever=30, top_k_reader=3)\n",
        "ensemble_prediction(input1,input2,test[qn])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/02/2020 09:44:15 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:44:15 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.91s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.91s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.94s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "08/02/2020 09:44:55 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:44:55 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.13s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.13s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.14s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[['S$810', 12.293428421020508], ['iv. S$810', 1.7623662948608398], ['S$810', 11.129851341247559]]\n",
            "[['S$810', 12.293428421020508], ['iv. S$810', 1.7623662948608398], ['S$810', 11.129851341247559]]\n",
            "[['S$810', 12.293428421020508], ['iv. S$810', 1.7623662948608398], ['S$810', 11.129851341247559]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('S$810', 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRXG0KInEqKq",
        "colab_type": "code",
        "colab": {},
        "outputId": "b526d20c-ac49-411d-a373-2ba97769f40c"
      },
      "source": [
        "#Consolidating predictions into a dataframe\n",
        "preds1 = []\n",
        "preds2 = []\n",
        "scores1 = []\n",
        "scores2 = []\n",
        "combinedPreds = []\n",
        "combinedScores=[]\n",
        "qns = list(test.keys())\n",
        "test1 = []\n",
        "for qn in qns:\n",
        "  #top_k_retriever indicates the maximum number of documents to be shortlisted by the retriever\n",
        "  #top_k_reader indicates the number of predictions to be returned\n",
        "    prediction1 = finder.get_answers(question=qn, top_k_retriever=30, top_k_reader=3)\n",
        "    prediction2 = finder2.get_answers(question=qn, top_k_retriever=30, top_k_reader=3)\n",
        "    pred1,score1 = best_prediction(prediction1,test[qn])\n",
        "    pred2,score2 = best_prediction(prediction2,test[qn])\n",
        "    combinedPred,combinedScore = ensemble_prediction(prediction1,prediction2,test[qn])\n",
        "    preds1.append(pred1)\n",
        "    preds2.append(pred2)\n",
        "    scores1.append(score1)\n",
        "    scores2.append(score2)\n",
        "    combinedPreds.append(combinedPred)\n",
        "    combinedScores.append(combinedScore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "08/02/2020 09:45:30 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:45:30 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.92s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.96s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.97s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.05s/ Batches]\n",
            "08/02/2020 09:46:10 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
            "08/02/2020 09:46:10 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "08/02/2020 09:46:20 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:46:20 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.96s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.07s/ Batches]\n",
            "08/02/2020 09:47:01 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.005s]\n",
            "08/02/2020 09:47:01 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.15s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.15s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:47:12 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.005s]\n",
            "08/02/2020 09:47:12 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.05s/ Batches]\n",
            "08/02/2020 09:47:52 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:47:52 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:48:03 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:48:03 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.06s/ Batches]\n",
            "08/02/2020 09:48:43 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
            "08/02/2020 09:48:43 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:48:54 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:48:54 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.08s/ Batches]\n",
            "08/02/2020 09:49:34 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:49:34 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:49:45 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:49:45 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.09s/ Batches]\n",
            "08/02/2020 09:50:26 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:50:26 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.19s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:50:36 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:50:36 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.09s/ Batches]\n",
            "08/02/2020 09:51:17 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
            "08/02/2020 09:51:17 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:51:27 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:51:27 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.05s/ Batches]\n",
            "08/02/2020 09:52:08 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:52:08 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:52:18 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:52:18 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.04s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.07s/ Batches]\n",
            "08/02/2020 09:52:59 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:52:59 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:53:09 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:53:09 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.98s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.11s/ Batches]\n",
            "08/02/2020 09:53:50 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.008s]\n",
            "08/02/2020 09:53:50 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:54:01 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.005s]\n",
            "08/02/2020 09:54:01 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.06s/ Batches]\n",
            "08/02/2020 09:54:41 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:54:41 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:54:52 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.005s]\n",
            "08/02/2020 09:54:52 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.04s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.10s/ Batches]\n",
            "08/02/2020 09:55:33 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:55:33 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:55:43 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:55:43 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.10s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "08/02/2020 09:56:24 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:56:24 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "08/02/2020 09:56:34 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:56:34 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.09s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.98s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "08/02/2020 09:57:14 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:57:14 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.19s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:57:25 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:57:25 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.07s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "08/02/2020 09:58:06 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:58:06 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.16s/ Batches]\n",
            "08/02/2020 09:58:16 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:58:16 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.98s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.08s/ Batches]\n",
            "08/02/2020 09:58:56 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:58:56 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:59:07 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 09:59:07 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.08s/ Batches]\n",
            "08/02/2020 09:59:48 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:59:48 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 09:59:58 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 09:59:58 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:09<00:00,  4.99s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.03s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.08s/ Batches]\n",
            "08/02/2020 10:00:39 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 10:00:39 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.19s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 10:00:49 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.011s]\n",
            "08/02/2020 10:00:49 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.00s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.07s/ Batches]\n",
            "08/02/2020 10:01:30 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 10:01:30 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "08/02/2020 10:01:40 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.007s]\n",
            "08/02/2020 10:01:40 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.01s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.02s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:10<00:00,  5.08s/ Batches]\n",
            "08/02/2020 10:02:21 - INFO - elasticsearch -   POST http://localhost:9200/document/_search [status:200 request:0.006s]\n",
            "08/02/2020 10:02:21 - INFO - haystack.finder -   Reader is looking for detailed answer in 177632 chars ...\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n",
            "Inferencing Samples: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ Batches]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IChfJh7hEqKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "types = []\n",
        "answers = []\n",
        "for qn in qns:\n",
        "    types.append(taskType[qn])\n",
        "for t in test:\n",
        "    #Retrieve the corresponding answers for each question\n",
        "    index = queries[queries['Question'] == t].index[0]\n",
        "    answers.append(queries['Answer'][index])\n",
        "    \n",
        "df = pd.DataFrame({'Question':qns,'Type':types,'Answer':answers,'Prediction1':preds1,'Score1':scores1,'Prediction2':preds2,'Score2':scores2,\n",
        "                  'OverallPrediction':combinedPreds,'OverallScore':combinedScores})\n",
        "df = df.sort_values(by=['OverallScore'])\n",
        "df.to_csv('results.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCgBGjhg8r5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Only two models can be used at once, so I combined both csv files before uploading them again to be processed\n",
        "models = ['Albert','Electra','Ensemble']\n",
        "scoreBreakdown = {models[0]:{'Overall':[]},models[1]:{'Overall':[]},models[2]:{'Overall':[]}}\n",
        "qTypes = {1:'Single Supporting Fact',6:'Yes/No',8:'Lists',9:'Simple Negation'}\n",
        "for index in range(len(df)):\n",
        "  scores = [df['Score1'][index],df['Score2'][index],df['OverallScore'][index]]\n",
        "  taskType = qTypes[df['Type'][index]]\n",
        "  for model,score in zip(models,scores):\n",
        "    currModel = scoreBreakdown[model]\n",
        "    if taskType not in currModel:\n",
        "      currModel[taskType] = []\n",
        "    currModel[taskType].append(score)\n",
        "    currModel['Overall'].append(score)\n",
        "for model,values in scoreBreakdown.items():\n",
        "  for taskType in values.keys():\n",
        "    values[taskType] = np.mean(values[taskType])\n",
        "scoreBreakdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwI6PGst94US",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "1e0e9269-4f0f-48a8-c111-d37ec65b58e5"
      },
      "source": [
        "#Choosing models to be combined into an ensemble.\n",
        "#We don't want to just choose models which have the best performance. Instead, we should choose pairs that are able to best compensate for each other's weaknesses \n",
        "# in predicting certain types of tasks. This is achieved by shortlisting model pairs which have the highest maximum score for each of the tasks.\n",
        "\n",
        "comparison = {'ALBERT':{'Overall':0.5745,1:0.5952,6:0.2899,8:0.5931,9:0.8488},'ROBERTA':{'Overall':0.5781,1:0.7437,6:0.2222,8:0.4477,9:0.3140},\n",
        "              'ELECTRA':{'Overall': 0.5471,1:0.6708,6:0.6667,8:0.1333,9:0.2465},'BERT':{'Overall':0.3472,1:0.4400,6:0.3939,8:0.1200,9:0.0612}}\n",
        "\n",
        "models = list(comparison.keys())\n",
        "import math\n",
        "output = []\n",
        "for i in range(len(models)-1):\n",
        "  for j in range(i+1,len(models)):\n",
        "    score = 0\n",
        "    for key in comparison[models[i]].keys():\n",
        "      if key != 'Overall':\n",
        "        score += max(comparison[models[i]][key],comparison[models[j]][key])\n",
        "    output.append('{}, {} have an expected compatibility of {}'.format(models[i],models[j],score))\n",
        "output.sort(key = lambda x:float(x.split(' ')[-1]),reverse=True)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ALBERT, ELECTRA have an expected compatibility of 2.7794',\n",
              " 'ALBERT, ROBERTA have an expected compatibility of 2.4755000000000003',\n",
              " 'ALBERT, BERT have an expected compatibility of 2.431',\n",
              " 'ROBERTA, ELECTRA have an expected compatibility of 2.1721',\n",
              " 'ROBERTA, BERT have an expected compatibility of 1.8993',\n",
              " 'ELECTRA, BERT have an expected compatibility of 1.7172999999999998']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}